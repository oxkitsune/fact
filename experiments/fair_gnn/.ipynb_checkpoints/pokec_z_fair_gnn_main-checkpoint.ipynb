{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36490633-6b99-4cda-bbf8-6fba5ebefb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using directories:\n",
      "root_dir: /home/fact21/fact_refactor\n",
      "data_dir: /home/fact21/fact_refactor/dataset/pokec\n",
      "log_dir: /home/fact21/fact_refactor/experiments/fair_gnn/logs/pokec_z\n",
      "========================================\n",
      "device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "import base_experiment\n",
    "\n",
    "\n",
    "\n",
    "ROOT_DIR, DATA_PATH, LOG_DIR, DEVICE = base_experiment.setup_experiment(\n",
    "    seed=36, \n",
    "    data_path=\"dataset/pokec\", \n",
    "    log_dir=\"experiments/fair_gnn/logs/pokec_z\", \n",
    "    device=1\n",
    ")\n",
    "\n",
    "# import these after, as we need to set correct path in setup_experiment\n",
    "import torch\n",
    "import numpy as np\n",
    "from models.fair import FairGNN, FairGNNTrainer\n",
    "from dataset import PokecZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85528069-1cf2-4e5a-8d97-56fde897e90a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.07 GiB (GPU 2; 23.64 GiB total capacity; 21.49 GiB already allocated; 746.50 MiB free; 21.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load in the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPokecZ\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregion_job.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43medges_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregion_job_relationship.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpokec_z_embedding10.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeat_drop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded dataset with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mnum_nodes()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nodes and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mnum_edges()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m edges\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing feat_drop_rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mfeat_drop_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/fact_refactor/src/dataset.py:339\u001b[0m, in \u001b[0;36mPokecZ.__init__\u001b[0;34m(self, nodes_path, edges_path, embedding_path, feat_drop_rate, device, sens_attr, predict_attr, label_number, sens_number, sample_number, test_idx, normalize_features, data_seed)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    325\u001b[0m     nodes_path: Path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m     data_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m    338\u001b[0m ):\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43medges_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeat_drop_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43msens_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredict_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43msens_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_user_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fact_refactor/src/dataset.py:164\u001b[0m, in \u001b[0;36mFairACDataset.__init__\u001b[0;34m(self, nodes_path, edges_path, embedding_path, feat_drop_rate, sens_attr, predict_attr, label_number, sens_number, sample_number, test_idx, normalize_features, data_seed, device, remove_user_id)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msub_keep_indices\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor(keep_indices, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msub_drop_indices\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor(drop_indices, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msub_adjs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 164\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madj\u001b[49m\u001b[43m[\u001b[49m\u001b[43msub_node\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_node\u001b[49m\u001b[43m]\u001b[49m[:, keep_indices]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    167\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(adj\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m    168\u001b[0m mask[train_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.07 GiB (GPU 2; 23.64 GiB total capacity; 21.49 GiB already allocated; 746.50 MiB free; 21.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Load in the dataset\n",
    "dataset = PokecZ(\n",
    "    nodes_path=DATA_PATH / \"region_job.csv\",\n",
    "    edges_path=DATA_PATH / \"region_job_relationship.txt\",\n",
    "    embedding_path=DATA_PATH / \"pokec_z_embedding10.npy\",\n",
    "    feat_drop_rate=0.3,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(f\"Loaded dataset with {dataset.graph.num_nodes()} nodes and {dataset.graph.num_edges()} edges\")\n",
    "print(f\"Using feat_drop_rate: {dataset.feat_drop_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8048c63-8ab5-4df1-9a3a-690b9b7a7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FairGNN model\n",
    "fair_gnn = FairGNN(\n",
    "    num_features=dataset.features.shape[1],\n",
    "    alpha=100, # for pokec_z, alpha is set to 100\n",
    ").to(DEVICE)\n",
    "\n",
    "# load pre-trained estimator\n",
    "estimator_path = ROOT_DIR / \"src/checkpoint/GCN_sens_region_job_ns_200\"\n",
    "fair_gnn.estimator.load_state_dict(torch.load(str(estimator_path.resolve())))\n",
    "\n",
    "print(f\"Created FairGNN model with {1} sensitive class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da444a-ddb7-4f23-88c0-1b7374308b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fair gnn trainer\n",
    "trainer = FairGNNTrainer(\n",
    "    dataset=dataset,\n",
    "    fair_gnn=fair_gnn,\n",
    "    device=DEVICE,\n",
    "    log_dir=LOG_DIR,\n",
    "    min_acc=0.65,\n",
    "    min_roc=0.69,\n",
    ")\n",
    "\n",
    "print(f\"Created trainer with {'GCN'} model, using LOG_DIR: {LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52df54-5a2d-43d5-b783-e7e172f6c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train(epochs=3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
